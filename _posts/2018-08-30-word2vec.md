---
layout:     post
title:      word2vec论文阅读
subtitle:   skip gram模型
date:       2018-08-30
author:     tracy
header-img: img/post-bg-BJJ.jpg
catalog: true
tags:
    - word2vec
---


## 前言

现在用深度学习做NLP，一定会用到word2vec，也就是词语的分布式表示。阅读了论文《Distributed Representations of Words and Phrases and their Compositionality》，记录一下。

## Skip Gram模型

skip gram模型的目标是找到最有利于预测周围词语的word representation，即给定一个序列$w_1, w_2, ..., w_T$，最大化下面的目标函数(即给定当前词语，预测上下文，与CBOW正好相反)：

$$ \frac{1}{T} \sum_{t=1}^{T}{\sum_{-c<=j<=c}{\textrm{log} p(w_{t+j} | w_t)}} $$

$c$是考虑的context的长度。关于上式中条件概率的计算方法，skip-gram使用的是基础的softmax函数：

$$ p(w_O | w_I) = \frac{\textrm{exp} ({v_{w_O}}^T v_{w_I})}{\sum_{w=1}^W \textrm{exp} ({v_w}^T v_{w_I})} $$

其中$v_w$和$v_w$分别是$w$的"input and output representation"，即每个词语有两个表示(?)。$W$是词汇表的大小，所以计算上式的复杂度是$O(W)$，$W$通常是非常大的。

## Hierarchical softmax

full softmax的一种计算量较小的近似，复杂度为$O(log_2 W)$。Hierarchical softmax使用二叉树结构来作为输出层，每个叶子节点代表一个单词，共有$W$个叶子。每个非叶子结点有一个向量表示。从根节点到一个叶子节点有唯一的路径，一个词语的概率，就等于从根节点开始随机走(random walk)，走到这个目标词语的概率。在每个非叶子结点，要知道向左走还是向右走的概率，因此hierarchical softmax的平均计算量是二叉树的高度，也就是$log_2 W$。

在word2vec的工作中，使用霍夫曼二叉树，把高频词放在离root近的叶结点。

## Negative sampling



“许多模型或方法可以用来获取词向量，除了word2vec使用的模型外，LSA矩阵分解模型、pLSA潜在语义分析模型、LDA文档生成模型，也均可用来估计词向量。”，LDA以前看过，有点忘了。。有时间也了解一下其他方法。