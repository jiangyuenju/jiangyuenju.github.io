---
layout:     post
title:      word2vec论文阅读
subtitle:   skip gram模型
date:       2018-08-30
author:     tracy
header-img: img/post-bg-BJJ.jpg
catalog: true
tags:
    - word2vec
---


## 前言

现在用深度学习做NLP，一定会用到word2vec，也就是词语的分布式表示。阅读了论文《Distributed Representations of Words and Phrases and their Compositionality》，记录一下。

## Skip Gram模型

skip gram模型的目标是找到最有利于预测周围词语的word representation，即给定一个序列$w_1, w_2, ..., w_T$，最大化下面的目标函数：

$$ \frac{1}{T} \sum_{t=1}^{T}{\sum_{-c<=j<=c}{\textrm{log} p(w_{t+j} | w_t)}} $$

$c$是考虑的context的长度。关于$ \textrm{log} p(w_{t+j} | w_t) $的计算方法，skip-gram使用的是基础的softmax函数：

$$ p(w_O | w_I) = /frac{/textrm{exp} ({v_{w_O}^'}^T v_{w_I})}{\sum_{w=1}^W /textrm{exp} ({v_w^'}^T v_{w_I})} $$

其中$v_w$和$v_w^'$分别是$w$的"input and output representation"，即每个词语有两个表示(?)。$W$是词汇表的大小，所以计算上式的复杂度是$O(W)$，$W$通常是非常大的。